## Train
### model
## 使用 LoRA 合并后的模型
model_name_or_path: output/qwen25vl_7B_stage3_sft
image_max_pixels: 262144
video_max_pixels: 16384
trust_remote_code: true

### method
stage: dpo
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all
pref_beta: 0.1 # 偏好损失中的 beta 参数
pref_loss: sigmoid  # choices: [sigmoid (dpo), orpo, simpo]


### dataset
dataset: iPersonal_GUI_stage3_dpo_shot_8_train
template: qwen2_vl
cutoff_len: 1000000 # 输入的最大 token 数, 超过该长度会被截断。
max_samples: 1000000 # 每个数据集的最大样本数, 超过将被截断。
overwrite_cache: true # 是否覆盖缓存的训练和评估数据集。
preprocessing_num_workers: 16 # 预处理时使用的进程数量。

### output
## TODO: 开始训练前修改 output_dir
output_dir: saves/qwen25vl_7B_stage3/lora/dpo
logging_steps: 100
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
num_train_epochs: 3.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 8 # 梯度积累步数
learning_rate: 5.0e-6
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
