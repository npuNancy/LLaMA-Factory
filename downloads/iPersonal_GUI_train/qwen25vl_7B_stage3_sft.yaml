## Train
### model
model_name_or_path: /data4/yanxiaokai/Models/modelscope/hub/Qwen/Qwen2.5-VL-7B-Instruct ## THU_LM1
image_max_pixels: 262144
video_max_pixels: 16384
trust_remote_code: true

### method
stage: sft
do_train: true # true用于训练, false用于评估
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_target: all

freeze_vision_tower: true # choices: [true, false] Whether ot not to freeze vision tower in MLLM training. default=True
freeze_multi_modal_projector: true # choices: [true, false] Whether or not to freeze the multi modal projector in MLLM training. default=True
train_mm_proj_only: false # choices: [true, false ]Whether or not to train the multimodal projector for MLLM only. default=False

### dataset
dataset: iPersonal_GUI_stage3_sft_100_train
template: qwen2_vl
cutoff_len: 1000000 # 输入的最大 token 数, 超过该长度会被截断。
max_samples: 1000000 # 每个数据集的最大样本数, 超过将被截断。
overwrite_cache: true # 是否覆盖缓存的训练和评估数据集。
preprocessing_num_workers: 16 # 预处理时使用的进程数量。

### output
# TODO: 开始训练前修改 output_dir
output_dir: saves/qwen25vl_7B_stage3/lora/sft
logging_steps: 100
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 2 # 梯度积累步数
learning_rate: 1.0e-4
num_train_epochs: 2
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

resume_from_checkpoint: saves/qwen25vl_7B_stage3/lora/sft # 断点重训

### eval
## 可以使用 eval_dataset 指定验证集, 也可以使用 val_size 指定验证集比例
val_size: 0.05 # 20000*0.05=1000
# eval_dataset: iPersonal_GUI_stage3_sft_100_test
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
# predict_with_generate: true ## 会调用 metrics 计算相似度
# load_best_model_at_end: true # 是否在训练结束后加载最佳模型。
