# The batch generation can be SLOW using this config.
# For faster inference, we recommend to use `scripts/vllm_infer.py`.

# ###############################################################
# CUDA_VISIBLE_DEVICES=6 python scripts/vllm_infer.py \
#     --model_name_or_path /data4/yanxiaokai/Models/modelscope/hub/Qwen/Qwen2.5-VL-7B-Instruct \
#     --adapter_name_or_path /data4/yanxiaokai/LLaMA-Factory/saves/qwen25vl_7B_stage2/lora/sft/checkpoint-27000 \
#     --template qwen2_vl \
#     --dataset iPersonal_GUI_stage2_aitw_len_1_test \
#     --save_name /data4/yanxiaokai/LLaMA-Factory/saves/qwen25vl_7B_stage2/lora/predict_aitw_len_1.jsonl
# ###############################################################


### model
model_name_or_path: /data4/yanxiaokai/Models/modelscope/hub/Qwen/Qwen2.5-VL-7B-Instruct
adapter_name_or_path: saves/qwen25vl_7B_stage2/lora/sft/checkpoint-27000
trust_remote_code: true


### method
stage: sft
do_predict: true
finetuning_type: lora
infer_backend: huggingface  # choices: [huggingface, vllm]

### dataset
# eval_dataset: iPersonal_GUI_stage2_aitw_len_1_test, iPersonal_GUI_stage2_aitw_len_2_test, iPersonal_GUI_stage2_aitw_len_3_test, iPersonal_GUI_stage2_aitw_len_4_test, iPersonal_GUI_stage2_aitw_len_5_test, iPersonal_GUI_stage2_android_control_len_1_test, iPersonal_GUI_stage2_android_control_len_2_test, iPersonal_GUI_stage2_android_control_len_3_test, iPersonal_GUI_stage2_android_control_len_4_test, iPersonal_GUI_stage2_android_control_len_5_test
eval_dataset: iPersonal_GUI_stage2_aitw_len_1_test
template: qwen2_vl
cutoff_len: 1000000 # 输入的最大 token 数, 超过该长度会被截断。
max_samples: 2000  # 每个数据集的最大样本数, 超过将被截断。 默认是全部数据集;
overwrite_cache: true  # 是否覆盖缓存的训练和评估数据集。
preprocessing_num_workers: 16  # 预处理时使用的进程数量。


### output
output_dir: saves/qwen25vl_7B_stage2/lora/predict
overwrite_output_dir: true


### eval
# per_device_eval_batch_size: 1
predict_with_generate: true ## 
# ddp_timeout: 180000000


