# The batch generation can be SLOW using this config.
# For faster inference, we recommend to use `scripts/vllm_infer.py`.

# ###############################################################
## vllm 推理 
# CUDA_VISIBLE_DEVICES=1 python scripts/vllm_infer.py \
#     --model_name_or_path /data4/yanxiaokai/Models/modelscope/hub/Qwen/Qwen2.5-VL-7B-Instruct \
#     --adapter_name_or_path saves/qwen25vl_7B_stage3/lora/sft_100user_20way_event/checkpoint-15000 \
#     --template qwen2_vl \
#     --gpu_memory_utilization 0.15 \
#     --vllm_gpu_util 0.15 \
#     --cutoff_len 10000 \
#     --max_samples 1000 \
#     --dataset iPersonal_GUI_stage3_sft_100user_20way_event_test \
#     --save_name saves/qwen25vl_7B_stage3/lora/predict_100user_20way_event.jsonl
# ###############################################################
## 推理+测试 
# CUDA_VISIBLE_DEVICES=XX lmf train downloads/iPersonal_GUI_inference/qwen25vl_7B_stage3.yaml
# ###############################################################


### model
model_name_or_path: /data4/yanxiaokai/Models/modelscope/hub/Qwen/Qwen2.5-VL-7B-Instruct
adapter_name_or_path: saves/qwen25vl_7B_stage3/lora/sft_100user_20way_event/checkpoint-15000
trust_remote_code: true


### method
stage: sft
do_predict: true
finetuning_type: lora
infer_backend: huggingface  # choices: [huggingface, vllm]

# 要添加到 tokenizer 中的 special token。多个 special token 用逗号分隔。
new_special_tokens: "<Role>,</Role>,<Task>,</Task>,<Format>,</Format>,<Rules>,</Rules>,<choices>,</choices>"

### dataset
eval_dataset: iPersonal_GUI_stage3_sft_100user_20way_event_test
template: qwen2_vl
cutoff_len: 10000 # 输入的最大 token 数, 超过该长度会被截断。
max_samples: 500  # 每个数据集的最大样本数, 超过将被截断。 默认是全部数据集;
overwrite_cache: true  # 是否覆盖缓存的训练和评估数据集。
preprocessing_num_workers: 16  # 预处理时使用的进程数量。


### output
output_dir: saves/qwen25vl_7B_stage3/lora/predict_100user_20way_event
overwrite_output_dir: true


### eval
# ddp_timeout: 180000000
per_device_eval_batch_size: 1
predict_with_generate: true ## 会调用 metrics 计算相似度 


