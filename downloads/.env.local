# Note: actually we do not support .env, just for reference

# 常用环境变量
# USE_MODELSCOPE_HUB=1 # 模型从 ModelScope 魔搭社区下载。避免从 HuggingFace 下载模型导致网速不畅。
# DISABLE_VERSION_CHECK=1 # 禁用版本检查, transformers 版本不兼容会导致报错。
# CUDA_VISIBLE_DEVICES=0,1 # 指定使用的 GPU 设备
# GRADIO_SERVER_PORT= # 指定 Gradio UI 的端口
# FORCE_TORCHRUN=1 # 分布式训练，基于PyTorch的NativeDDP

# 常用命令
## 训练
# 单卡 CUDA_VISIBLE_DEVICES=XX lmf train path/to/config.yaml
# 多卡 FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=XX,XX,XX lmf train path/to/config.yaml > /data4/yanxiaokai/LLaMA-Factory/saves/qwen25vl_7B_stage2/lora/train_log.txt 2>&1
## 推理|测试
# 测试 CUDA_VISIBLE_DEVICES=XX lmf train path/to/config.yaml
"""
CUDA_VISIBLE_DEVICES=XX python scripts/vllm_infer.py \
    --model_name_or_path path_to_merged_model \ 
    --adapter_name_or_path path_to_adapter \
    --dataset dataset_name \
    --cutoff_len 1000000 \
    --max_samples 1000 \
    --template qwen2_vl \
    --save_name path/to/save.jsonl
"""

# api
API_HOST=
API_PORT=
API_KEY=
API_MODEL_NAME=
FASTAPI_ROOT_PATH=
MAX_CONCURRENT=
# general
DISABLE_VERSION_CHECK=
FORCE_CHECK_IMPORTS=
ALLOW_EXTRA_ARGS=
LLAMAFACTORY_VERBOSITY=
USE_MODELSCOPE_HUB=
USE_OPENMIND_HUB=
USE_RAY=
RECORD_VRAM=
# torchrun
FORCE_TORCHRUN=
MASTER_ADDR=
MASTER_PORT=
NNODES=
NODE_RANK=
NPROC_PER_NODE=
# wandb
WANDB_DISABLED=
WANDB_PROJECT=
WANDB_API_KEY=
# gradio ui
GRADIO_SHARE=
GRADIO_SERVER_NAME=
GRADIO_SERVER_PORT=
GRADIO_ROOT_PATH=
GRADIO_IPV6=
# setup
ENABLE_SHORT_CONSOLE=
# reserved (do not use)
LLAMABOARD_ENABLED=
LLAMABOARD_WORKDIR=
